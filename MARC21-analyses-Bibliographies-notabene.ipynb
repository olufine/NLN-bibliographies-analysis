{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses of MARC21 metadata records - bibliographies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains code for the analyses described in the paper \n",
    "_Ohren, O. Getting meaning out of metadata - analysis of selected bibliographies at the National Library of Norway. Oslo 2024_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "import re #regular expressions\n",
    "import requests\n",
    "import urllib, urllib.parse     # used for percent-encoding strings\n",
    "import xml\n",
    "from xml import etree\n",
    "from xml.etree import ElementTree\n",
    "from io import StringIO\n",
    "import pymarc\n",
    "from pymarc import Record, marcxml, Field, XMLWriter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "import numpy\n",
    "import unicodedata as ucd\n",
    "# debugging\n",
    "import pdb\n",
    "import traceback\n",
    "#data storage\n",
    "import sqlite3\n",
    "from wordcloud import WordCloud\n",
    "#for Analysis 5 (geomapping)\n",
    "import geocoder\n",
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "repopath=os.path.abspath('../Gitrepos/tkpy')\n",
    "if repopath not in sys.path:\n",
    "    sys.path.append(repopath)\n",
    "import harvest, iomarc, iogeneral, marcpy1, marcpy2\n",
    "from marcpy1 import valueCounter, leaderValues, similar\n",
    "from marcpy2 import select, selectAssigned, filterRecordsByControlField, filterRecordsByLeader, filterRecords\n",
    "from marcpy2 import fetchRecordSimple, indexRecords\n",
    "from iomarc import printFields, printFieldss, writeMarcToFile \n",
    "from iogeneral import writeToFile, readlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities and string functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(strng):\n",
    "    #converts strn to a string without newlines and without extra spaces\n",
    "    strlst=strng.splitlines()     #remove \\n\n",
    "    result=''\n",
    "    for s in strlst:\n",
    "        result = result + ' ' + s.strip()\n",
    "    return result.strip()\n",
    "\n",
    "def oneLineStr(s):\n",
    "    #returns a one line version of s. I.e. without line separators, and without leading or trailing spaces on\n",
    "    #    each line of s.\n",
    "    return ''.join(list(map(lambda x: x.strip() + ' ', s.splitlines()))).strip()\n",
    "\n",
    "def reverseDict (d):\n",
    "    #returns a new dict where the keys are the values of d, and the values lists of the keys of d\n",
    "    #assumes that the values of d are strings or numbers (unmutable)\n",
    "    r=dict()\n",
    "    for k1 in d.keys():\n",
    "        if type(d[k1]) in [str, int, float] and d[k1] not in r.keys():\n",
    "            r[d[k1]] = [k1]\n",
    "            for k2 in d.keys():\n",
    "                if d[k1]==d[k2] and k2 not in r[d[k1]]:\n",
    "                    r[d[k1]].append(k2)\n",
    "    return r\n",
    "\n",
    "def mergeDicts(dictionaries, nullVal=0):\n",
    "    #returns a Dict which is a merge between the dictionaries in dictionaries \n",
    "    #the values of the merged dict is a list containing  the values of  the dictionaries in \n",
    "    #dictionaries separately. Becomes a matrix\n",
    "    #nullVal is the empty value\n",
    "    #collect all keys\n",
    "    allkeys=[]\n",
    "    for d in dictionaries:\n",
    "        allkeys=list(set(allkeys).union(set(list(d))))\n",
    "    #Create the dictionary\n",
    "    merged=dict()\n",
    "    allkeys.sort()\n",
    "    #Initialize\n",
    "    for k in allkeys:\n",
    "        merged[k]=[]\n",
    "    #Collect values from dictionaries\n",
    "    for k in sorted(allkeys):\n",
    "        for d in dictionaries:\n",
    "            if k in list(d):\n",
    "                merged[k].append(d[k])\n",
    "            else:\n",
    "                merged[k].append(nullVal)\n",
    "    return merged\n",
    "\n",
    "def reduceNumDict(dictionary, threshold, aggrKey='other'):\n",
    "    #Assumes dict has numeric values, e.g. {'nor':43, 'dan':5, etc}\n",
    "    #All values less than threshold is summed up and assigned to a key aggrKey\n",
    "    #Returns a new dict with all values under thresjhold are assigned to aggrKey\n",
    "    res=dict()\n",
    "    aggrval=0\n",
    "    for k in dictionary.keys():\n",
    "        if dictionary[k]<threshold:\n",
    "            aggrval+=dictionary[k]\n",
    "        else:\n",
    "            res[k]=dictionary[k]\n",
    "    res[aggrKey]=aggrval\n",
    "    return res\n",
    "\n",
    "def flatten(l):\n",
    "    #returns a list of all the elements of all the lists in l. (Flattened 1 level)\n",
    "    result=[]\n",
    "    for elt in l:\n",
    "        if type(elt)==list:\n",
    "            result.extend(elt)\n",
    "        else:\n",
    "            result.append(elt)\n",
    "    return result\n",
    "\n",
    "def compress(seq, toRemove):\n",
    "    #Returns a cpopy of seq (list or tuple) without any of the elements in toRemove\n",
    "    result = []\n",
    "    for elt in seq:\n",
    "        if elt not in toRemove:\n",
    "            result.append(elt)\n",
    "    if type(seq) == tuple:\n",
    "        return tuple(result)\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def removePrefixes(l):\n",
    "    #returns a new list with all the elements in l which are not prefixes or equal another element\n",
    "    #only 1 level is handled (no nesting)\n",
    "    #meant to be used for lists of strings.\n",
    "    #non-strings elements are converted to strings\n",
    "    deleted=[]\n",
    "    for e1 in l:\n",
    "        for e2 in l:\n",
    "            if e1 not in deleted and str(e2).startswith(str(e1)) and len(str(e1))< len(str(e2)):\n",
    "                deleted.append(e1)\n",
    "    return list(set(l).difference(set(deleted)))\n",
    "\n",
    "def iscapitalized(strng):\n",
    "    #Returns True iff strng starts with an uppecase letter and the rest (of the cased letters) are lowecase\n",
    "    cap=False\n",
    "    if strng!='':\n",
    "        if strng[0].isupper():\n",
    "            if len(strng) > 1:\n",
    "                if strng[1:].islower() or strng.endswith('.'):\n",
    "                    cap=True\n",
    "            else:\n",
    "                cap=True\n",
    "    return cap\n",
    "\n",
    "def transpose(lstlst):\n",
    "    #lstlst is a list of sequences.\n",
    "    #returns a list of lists, in which the internal lists are transpositions of input\n",
    "    #lstlst=[[1,2,3], [4,5,6], [7,8,9,10]]\n",
    "    #returns[[1,4,7], [2,5,8], [3,6,9]]\n",
    "    min_l=min(list(map(lambda x: len(x), lstlst)))\n",
    "    result=[]\n",
    "    for i in range(0,min_l):\n",
    "        comp=[]\n",
    "        for seq in lstlst:\n",
    "            comp.append(seq[i])\n",
    "        result.append(comp)    \n",
    "    return result    \n",
    "\n",
    "def sum(lst):\n",
    "    res=0\n",
    "    for i in lst:\n",
    "        res+=i\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography-specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(bibl1, codebibl2):\n",
    "    #Returns the overlap between the dataset bibl1 and another dataset indicated by 913$a<codebibl2>\n",
    "    #That is, the records in bibl1 for which 913$a<codebibl2> exist\n",
    "    #Tolerate 1st character lower and uppercase\n",
    "    r='('+codebibl2[0].lower() + '|' + codebibl2[0].upper() + ')' + codebibl2[1:] #Parentheses are necessary!\n",
    "    if bibl1 !=[]:\n",
    "        result=filterRecords(bibl1, r,['913'])\n",
    "    else:\n",
    "        result= []\n",
    "    return result\n",
    "\n",
    "def authorGender(autrecs, biblrecs):\n",
    "    #returns a list of 3 lists:\n",
    "    #1. the biblrecs with female main authors\n",
    "    #2. the biblrecs with male authors\n",
    "    #3. the biblrecs with no gender info on main author\n",
    "    #(biblrecs minus the union of the 3 above include records that have no main author\n",
    "    #    or have main author, but no author ID, or the author ID is not found in autrecs)\n",
    "    females=[]\n",
    "    males=[]\n",
    "    noGenderInfo=[]\n",
    "    #Create an index of autrecs, for more efficient retrieval\n",
    "    autindx=indexRecords(autrecs)\n",
    "    #Look only at those with 100$0 field (has main author (person) and is authorised)\n",
    "    withMainAuth=selectAssigned(biblrecs,'100', subfields=['0'])\n",
    "    for rec in withMainAuth:\n",
    "        #remove the prefix from $0\n",
    "        autid = rec.get_fields('100')[0].get_subfields('0')[0][10:]\n",
    "        if autid in autindx.keys():\n",
    "            aut=autindx[autid]\n",
    "            gf=aut.get_fields('375')\n",
    "            if gf!=[]:\n",
    "                if gf[0].value()[0] in {'f', 'F'}:\n",
    "                    females.append(rec)\n",
    "                elif gf[0].value()[0] in {'m', 'M'}: \n",
    "                    males.append(rec)\n",
    "            else:\n",
    "                noGenderInfo.append(rec)\n",
    "        else:\n",
    "            noGenderInfo.append(rec)\n",
    "    return [females, males, noGenderInfo]\n",
    "\n",
    "def authorGender2(biblrecs, girls, boys):\n",
    "    #returns a list of 3 lists:\n",
    "    #1. the biblrecs with female main authors\n",
    "    #2. the biblrecs with male authors\n",
    "    #3. the biblrecs with no gender info on main author\n",
    "    #girls and bouys are  lists of names extracted from SSB (https://data.ssb.no/api/v0/no/console)\n",
    "    females=[]\n",
    "    males=[]\n",
    "    noGenderInfo=[]\n",
    "    #Look only at those with 100$a field (has main author (person) and a name)\n",
    "    withMainAuth=selectAssigned(biblrecs,'100', subfields=['a'])\n",
    "    #Identify the individual first names in 100$a\n",
    "    for rec in withMainAuth:\n",
    "        names= forenames(rec.get_fields('100')[0].get_subfields('a')[0])\n",
    "        if set(girls).intersection(set(names)) != set():\n",
    "            females.append(rec)\n",
    "        elif set(boys).intersection(set(names)) != set():\n",
    "            males.append(rec)\n",
    "        else:\n",
    "            noGenderInfo.append(rec)\n",
    "    return [females, males, noGenderInfo]\n",
    "\n",
    "def forenames (namestring):\n",
    "    #returns a list of forenames from a field 100a\n",
    "    #on the form <forenames>, <last name>(s), e.g. \n",
    "    # Kvamme, Ole Andreas   --> returns [Ole, Andreas]\n",
    "    # Downs, Brian H.  ---> returns [Brian]\n",
    "    fnamestr=namestring.partition(',')[2].strip()\n",
    "    fnames=list(map (lambda x: x.strip(), fnamestr.split(' ')))\n",
    "    #Ignore abbreviations/initials\n",
    "    res=[]\n",
    "    for s in fnames:\n",
    "        if len(s)>1 and s[-1]!='.':\n",
    "            res.append(s)\n",
    "    return res      \n",
    "    \n",
    "\n",
    "def publishedYears(records, groupSz=0):\n",
    "    yrCounter=valueCounter(records, ['008'], slice=(7,11))   #sorted by keys (years)\n",
    "    return pd.DataFrame(yrCounter.values(), index=yrCounter.keys())\n",
    "\n",
    "def publishedBetween(records, fromYear=0, toYear=2040):\n",
    "    #returns the number of records in records published in the given interval\n",
    "    yrCounter=valueCounter(records, ['008'], slice=(7,11))   #sorted by keys (years)\n",
    "    res=0\n",
    "    for k in yrCounter.keys():\n",
    "        if k.isdigit() and int(k)>=fromYear and int(k)<toYear:\n",
    "            res+=yrCounter[k]\n",
    "    return res\n",
    "\n",
    "def textvolume (records):\n",
    "    return textvolumeInfo(records)[2]\n",
    "\n",
    "def textvolumeInfo (records):\n",
    "    #calculates the approximate, total  number of pages or leaves in records\n",
    "    #Filters out the subset of records with 'a' in Leader\n",
    "    #Then calculates the number of pages or leaves from 300$a in the subset\n",
    "    #Returns a tuple of 3 elements: \n",
    "    #(1)The number of records, (2)the number of text records, (3)the number of text pages or leaves\n",
    "    #textrecs=filterRecordsByControlField(records, 'ta', '007', (0,2))\n",
    "    textrecs=filterRecordsByLeader(records, 'a', posint=(6,7))\n",
    "    textvol=sum(list(map (lambda x: textExtent(x), textrecs)))\n",
    "    return (len(records), len(textrecs), textvol)\n",
    "\n",
    "def textExtent(record):\n",
    "    ext=0\n",
    "    extentstr=''\n",
    "    f300=record.get_fields('300')\n",
    "    if f300 != []:\n",
    "        sf300a=f300[0].get_subfields('a')\n",
    "        if sf300a != []:\n",
    "            extentstr=sf300a[0]\n",
    "            ext=gatherTextExtent(extentstr)\n",
    "    return ext\n",
    "\n",
    "def gatherTextExtent(extentstring):\n",
    "    #extentstring is the total content of 300a\n",
    "    extentstr=extentstring\n",
    "    ext=0\n",
    "    if re.search('(\\d b\\. i 1)', extentstr) is not None:\n",
    "        #remove this, the rest should detail the pages\n",
    "        extentstr=extentstr.replace(re.search('(\\d b\\. i 1)', extentstr).groups()[0],'',1)\n",
    "    elif re.search('(\\d b\\.)', extentstr) is not None:\n",
    "        extentstr=extentstr.replace(re.search('(\\d b\\.)', extentstr).groups()[0],'',1)\n",
    "    for extentcomp in extentstr.split(','):\n",
    "            ext+=calcTextExtent(extentcomp)\n",
    "    return ext\n",
    "\n",
    "def calcTextExtent(extentstring):\n",
    "    #Calculates the number of pages or leaves expressed by extsentring\n",
    "    #extentstring is 1 statement in 300$a (which may contain several statements separated by comma)\n",
    "    #Example of 300$a: 1 bl., 4,  [2] s., S. 595-1088, [2] s. This contains 5 extentstatments, \n",
    "    #to be processed separately here\n",
    "    #examples: \n",
    "    # 148 s.| 150 s.|154 bl.|126 s.|'S. 95-96|Side 95-96 | S. 96-[118]|S. [103]-130| S. [109]-[121]\n",
    "    # V|[6] | 220 s.|\n",
    "    ext=0\n",
    "    #1. Detect number of units like 15 s. (or S.) or 15 sider (or Sider) or 15 bl. or Bl. or blad or Blad.\n",
    "    if re.search('\\[?(\\d+)\\]?\\s*((s|S)\\.|(s|S)ider|(b|B)l\\.|(b|B)lad)', extentstring) is not None:\n",
    "        #retrieve the first matching pagenumber\n",
    "        pagenum=re.search('(\\d+)', extentstring).groups()[0]\n",
    "        if pagenum.isnumeric() == True:\n",
    "            ext+=int(pagenum)\n",
    "    #2 Detect spans,  like 'S. 67 | S. 95-96|Side 95-96 | S. 96-[118]|S. [103]-130| S. [109]-[121]\n",
    "    elif re.search('((s|S)\\.|(s|S)ide)\\s*\\[?(\\d+)\\]?\\s*-\\s*\\[?(\\d+)', extentstring) is not None:\n",
    "        pagespan=re.search('(\\d+)[^\\d]*(\\d+)', extentstring).groups()\n",
    "        #print(pagespan)\n",
    "        if pagespan[0].isnumeric() == True and pagespan[1].isnumeric() == True:\n",
    "            ext+=int(pagespan[1])-int(pagespan[0])\n",
    "    #3 Detect span without unit in front (occurs in cases when 300a includes e.g. 's. [1]-284, 285-467',)\n",
    "    elif re.search('(\\d+)\\]?\\s*-\\s*\\[?(\\d+)', extentstring) is not None:\n",
    "        pagespan=re.search('(\\d+)[^\\d]*(\\d+)', extentstring).groups()\n",
    "        #print(pagespan)\n",
    "        if pagespan[0].isnumeric() == True and pagespan[1].isnumeric() == True:\n",
    "            ext+=int(pagespan[1])-int(pagespan[0])\n",
    "    #3 Detect single pages, like S. 67\n",
    "    elif re.search('((s|S)\\.|(s|S)ide)\\s*\\[?(\\d+)', extentstring) is not None:\n",
    "        ext+=1\n",
    "    #4 Detect pagenum without unit, like in 134\n",
    "    elif re.search('(\\d+)', extentstring) is not None:\n",
    "        #Assume this is a numer of pages or leaves (occurs in cases like  300a='134, 56 s.'')\n",
    "        pagenum=re.search('(\\d+)', extentstring).groups()[0]\n",
    "        if pagenum.isnumeric() == True:\n",
    "            ext+=int(pagenum) \n",
    "    return ext        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ny innlesing pga mistenkt feil i 600-felter\n",
    "\n",
    "handler=pymarc.marcxml.XmlHandler()\n",
    "solstad=pymarc.marcxml.parse_xml_to_array('solstad2022.xml') #actually returns a list\n",
    "proysen=pymarc.marcxml.parse_xml_to_array('proysen2022.xml')\n",
    "hamsun=pymarc.marcxml.parse_xml_to_array('hamsun2022.xml')\n",
    "collett=pymarc.marcxml.parse_xml_to_array('collett2022.xml')\n",
    "bjornson=pymarc.marcxml.parse_xml_to_array('bjornson2022.xml')\n",
    "norskeboker=pymarc.parse_xml_to_array('norske-boker-1519-1850_2022.xml')\n",
    "bibl1814=pymarc.parse_xml_to_array('1814_2022.xml')\n",
    "noram=pymarc.parse_xml_to_array('noram2022.xml')\n",
    "samisk=pymarc.parse_xml_to_array('samisk2022.xml')\n",
    "bibliografier=[solstad,proysen, hamsun, collett, bjornson, norskeboker, bibl1814, noram, samisk]\n",
    "bibliografiNavn={'solstad':solstad, 'prøysen': proysen , \n",
    "                 'hamsun':hamsun, 'collett':collett, 'bjørnson': bjornson, \n",
    "                 'norske bøker': norskeboker, '1814': bibl1814, 'norsk-amerikansk': noram, 'samisk': samisk}\n",
    "bibliografiVar={'solstad':'solstad', 'prøysen': 'proysen' , \n",
    "                 'hamsun':'hamsun', 'collett':'collett', 'bjørnson': 'bjornson', \n",
    "                 'norske bøker': 'norskeboker', '1814': 'bibl1814', 'norsk-amerikansk': 'noram', 'samisk': 'samisk'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bibliografiNavn.keys()):\n",
    "    print(k+': ', len(bibliografiNavn[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inspect and analyse the bibliographies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas cheat sheets: \n",
    "https://www.dataquest.io/blog/pandas-cheat-sheet/\n",
    "https://towardsdatascience.com/my-python-pandas-cheat-sheet-746b11e44368\n",
    "\n",
    "User guide:\n",
    "https://pandas.pydata.org/docs/user_guide/10min.html  (intro)\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html (IO facilities)\n",
    "https://pandas.pydata.org/docs/user_guide/text.html  (working with text)\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html (visualization, plotting)\n",
    "\n",
    "MatPlotLib:\n",
    "User guide:\n",
    "https://matplotlib.org/users/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Overview, number of records, pages, material type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bLength=list(map(lambda x: len(x), bibliografier))\n",
    "overview= pd.Series(bLength, index=bibliografiNavn.keys())\n",
    "df_overview=pd.DataFrame(overview, columns=['Antall poster'])\n",
    "fig0=df_overview.plot(kind='bar', legend=False, fontsize=16, figsize=(12,7)).get_figure()\n",
    "fig0.savefig('overview.pdf', bbox_inches='tight')\n",
    "fig0.savefig('overview.jpeg', bbox_inches='tight')\n",
    "#df_overview.plot(kind='bar', legend=False, fontsize=14, figsize=(8,4)).get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Type of material (medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts=[]\n",
    "for bn in bibliografiNavn.keys():\n",
    "    dicts.append(valueCounter(bibliografiNavn[bn], ['000'], slice=(6,7)))\n",
    "mediaTypes=mergeDicts(dicts)\n",
    "mediaTypes\n",
    "#Only distinguish between language material and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider only types a, c, i and j. The rest is summed up into 'other'\n",
    "#This is reduction based on keys, not on values\n",
    "dictsReduced=[]\n",
    "for bn in bibliografiNavn.keys():\n",
    "    tmp=valueCounter(bibliografiNavn[bn], ['000'], slice=(6,7))\n",
    "    reduced=dict()\n",
    "    keep=['a', 'c', 'i', 'j']\n",
    "    for k in keep:\n",
    "        reduced[k]=tmp[k]\n",
    "    #Sum up the rest\n",
    "    oth=0\n",
    "    for k in tmp.keys():\n",
    "        if k not in keep:\n",
    "            oth+=tmp[k]\n",
    "    reduced['other']=oth\n",
    "    dictsReduced.append(reduced)\n",
    "mediaTypesRed=mergeDicts(dictsReduced)\n",
    "mediaTypesRed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list (map (lambda x: list(x.values()), dictsReduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list (map (lambda x: list(x.values()), dictsReduced))\n",
    "df_media=pd.DataFrame(data, \n",
    "                      columns=['Text', 'Sheet music', 'Audio (not music)', 'Audio (music)', 'Other'], \n",
    "                      index=bibliografiNavn.keys())\n",
    "\n",
    "s=df_media.style\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(collett)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Size of textual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bPages=list(map(lambda x: textvolume(x), bibliografier))\n",
    "overview2= pd.Series(bPages, index=bibliografiNavn.keys())\n",
    "df_overview2=pd.DataFrame(overview2, columns=['Antall sider/blad'])\n",
    "fig02=df_overview2.plot(kind='bar', legend=False, fontsize=16, figsize=(12,7)).get_figure()\n",
    "fig02.savefig('overview2.pdf', bbox_inches='tight')\n",
    "fig02.savefig('overview2.jpeg', bbox_inches='tight')\n",
    "#df_overview2.plot(kind='bar', legend=False, fontsize=14, figsize=(8,5)).get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criterium for text 007/00-01 = ta (Do not use! This does not include text in computer filesMore than this is text)\n",
    "for bn in bibliografiNavn.keys():\n",
    "    size=textvolumeInfo(bibliografiNavn[bn])\n",
    "    pagesPerRecord=int(size[2]/size[0])\n",
    "    pagesPerTextRecord=int(size[2]/size[1])\n",
    "    print(bn, size, pagesPerRecord, 'per record', pagesPerTextRecord, 'per textual record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criterium for text: Using LDR/06 = 'a' (includes printed and digital text, but not audiobooks)\n",
    "pagesPerRecList=[]\n",
    "for bn in bibliografiNavn.keys():\n",
    "    size=textvolumeInfo(bibliografiNavn[bn])\n",
    "    pagesPerRecord=int(size[2]/size[0])\n",
    "    pagesPerTextRecord=int(size[2]/size[1])\n",
    "    pagesPerRecList.append(pagesPerTextRecord)\n",
    "    #print(bn, size, pagesPerRecord, 'per record', pagesPerTextRecord, 'per textual record')\n",
    "df_pagePerTextrec=pd.DataFrame(pagesPerRecList, \n",
    "                      columns=['Number of pages per text resource'],\n",
    "                      index=bibliografiNavn.keys()\n",
    "                    )\n",
    "\n",
    "s=df_pagePerTextrec\n",
    "s    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figpgPerRec=df_pagePerTextrec.plot(kind='line', legend=False, fontsize=16, figsize=(12,7), rot=270).get_figure()\n",
    "figpgPerRec.savefig('pgPerTextRec_line.pdf', bbox_inches='tight')\n",
    "figpgPerRec.savefig('pgPerTextRec_line.jpeg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Bibliographical level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliografisk nivå: Monografier eller artikler/kapitler?\n",
    "\n",
    "bibliografiskNiva=[]\n",
    "for bn in bibliografiNavn.keys():\n",
    "    b=bibliografiNavn[bn]\n",
    "    mono=filterRecordsByLeader(b,'m', (7,8))\n",
    "    comp=filterRecordsByLeader(b,'a', (7,8))\n",
    "    ser=filterRecordsByLeader(b,'s', (7,8))\n",
    "    integr=filterRecordsByLeader(b,'i', (7,8))\n",
    "    bibliografiskNiva.append((bn, len(mono), len(comp), len(ser), len (integr), len(b)))\n",
    "\n",
    "for tp in bibliografiskNiva:\n",
    "    print (tp[0]+':','\\t', 'Monografier:', tp[1],'('+ str(round((100*tp[1])/tp[5]))+'%)', '\\t', \n",
    "           'Artikler/Kapitler:', tp[2],'('+ str(round((100*tp[2])/tp[5]))+'%)','\\t', 'Serier:', tp[3], \n",
    "           '\\t','Hele bibliografien:', tp[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bibliografisk nivå: Monografier eller artikler/kapitler? Bruker DataFrame\n",
    "\n",
    "bibliografiskNiva=[]\n",
    "for bn in bibliografiNavn.keys():\n",
    "    b=bibliografiNavn[bn]\n",
    "    mono=filterRecordsByLeader(b,'m', (7,8))\n",
    "    comp=filterRecordsByLeader(b,'a', (7,8))\n",
    "    ser=filterRecordsByLeader(b,'s', (7,8))\n",
    "    integr=filterRecordsByLeader(b,'i', (7,8))\n",
    "    bibliografiskNiva.append([len(mono), len(comp), len(ser)+len(integr), len(b)])\n",
    "#Slå sammen serier og integrerte\n",
    "df_bibliografiskNiva=pd.DataFrame(bibliografiskNiva, index=bibliografiNavn.keys(),\n",
    "                                  columns=['Monographs', 'Articles/Chapters', 'Other','Total number'])\n",
    "df_bibliografiskNiva=df_bibliografiskNiva.T\n",
    "blevel=df_bibliografiskNiva[:-1].T.plot(kind='bar', stacked=True, fontsize=24, \n",
    "                                      title='Bibliographic level (absolute values)', \n",
    "                                      legend=False,figsize=(12,8)).get_figure()\n",
    "plt.legend(bbox_to_anchor=(1,1), fontsize='xx-large')\n",
    "blevel.savefig('biblLevelAbs.pdf', bbox_inches='tight')\n",
    "blevel.savefig('biblLevelAbs.jpeg', bbox_inches='tight')\n",
    "#Make df with relative distribution of material type\n",
    "reldf=pd.DataFrame(columns=bibliografiNavn.keys(), \n",
    "                               index=['Monographs', 'Articles/Chapters', 'Other', 'Total number '])\n",
    "for bn in bibliografiNavn.keys():\n",
    "    reldf[bn][:-1] = round((df_bibliografiskNiva[bn][:-1] / df_bibliografiskNiva[bn][-1]) * 100, 0)\n",
    "    reldf[bn][-1]=100  #df_bibliografiskNiva[bn][-1]\n",
    "blevelx=reldf[:-1].T.plot(kind='bar', stacked=True, fontsize=24, title='Bibliographic level (relative distribution)', legend=False,\n",
    "                       figsize=(12,8), rot=270).get_figure()\n",
    "blevelx.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1,1), fontsize='xx-large')\n",
    "blevelx.savefig('biblLevel.pdf', bbox_inches='tight') # the latter must be included to include the legend in the file\n",
    "blevelx.savefig('biblLevel.jpeg', bbox_inches='tight')\n",
    "\n",
    "#Line diagrame, to compare with numper of pages per text record. \n",
    "# Only include monographs portion (= 1st value in each reldf[bn])\n",
    "monogr_percentage=[list(map(lambda x: reldf[x][0],bibliografiNavn.keys()))]\n",
    "monogr_df=pd.DataFrame(data=monogr_percentage, columns=bibliografiNavn.keys()) \n",
    "mlevelx_line=monogr_df.T.plot(kind='line', fontsize=24, title='Percentage of monographs', legend=True,\n",
    "                       figsize=(12,8), rot=270).get_figure()   #rot=270 gives vertical Xticks labels\n",
    "mlevelx_line.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1,1), fontsize='xx-large')\n",
    "mlevelx_line.savefig('monogrLevelLine.pdf', bbox_inches='tight') # the latter must be included to include the legend in the file\n",
    "mlevelx_line.savefig('monogrLevelLine.jpeg', bbox_inches='tight')\n",
    "reldf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Overlap between data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The datasets are parsed separately, hence set intersection functions of no use\n",
    "\n",
    "#Er det poster som finnes i alle bibliografiene? Svar: 0\n",
    "overlapAll=bibliografiNavn[list(bibliografiNavn.keys())[0]]\n",
    "for i in range(0, len(bibliografiNavn.keys())):\n",
    "    if i<len((bibliografiNavn.keys())) and overlapAll != []:\n",
    "        k2=list(bibliografiNavn.keys())[i+1] #bibliografiNavn.keys() is not subscriptable\n",
    "        overlapAll=overlap(overlapAll, k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibliografiNavn[list(bibliografiNavn.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pairwise overlap\n",
    "overlapPairs=[]\n",
    "for bn1 in bibliografiNavn.keys():\n",
    "    overl=[]\n",
    "    for bn2 in bibliografiNavn.keys():\n",
    "        overl.append(len(overlap(bibliografiNavn[bn1], bn2)))\n",
    "    overlapPairs.append(overl)\n",
    "\n",
    "overlapsFrame=pd.DataFrame(overlapPairs, index=bibliografiNavn.keys(),columns=bibliografiNavn.keys())\n",
    "overlapsFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(map(lambda x: len(x), bibliografier))\n",
    "#print(145/4200)\n",
    "x=overlap(bibliografiNavn['norske bøker'], '1814')\n",
    "printFields(x, ['100','008', '245', '650'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove overlap with same bibliography\n",
    "for k in bibliografiNavn.keys():\n",
    "    overlapsFrame[k][k]=0\n",
    "fig2=overlapsFrame.plot(kind='bar', title='Overlap between bibliographies', legend=False, figsize=(14,10),\n",
    "                        fontsize=24, stacked=True).get_figure()\n",
    "#fig2.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1,1), fontsize='xx-large')\n",
    "#fig1.savefig('mtype.pdf', bbox_inches='tight')\n",
    "fig2.savefig('overlap.pdf', bbox_inches='tight')\n",
    "fig2.savefig('overlap.jpeg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Studying the authors and contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree of unauthorised responsibles\n",
    "\n",
    "def unauthorisedAgentsInfo(records, fieldtags, autrefSubfield):\n",
    "    #Calculates the proportion (in %) of records having a field with tag in fieldtags \n",
    "    #    that do not have the subfield autrefSubfield\n",
    "    #returns a tuple (number of records with unauth fieldtags (any), number of records with fieldtags (any), ratio)\n",
    "    unauthLst=[]\n",
    "    withFldLst=[]\n",
    "    for fld in fieldtags:\n",
    "        withFld=selectAssigned(records,fld)\n",
    "        unauth=list(set(withFld).difference(set(selectAssigned(records, fld, autrefSubfield))))\n",
    "        withFldLst.extend(withFld)\n",
    "        unauthLst.extend(unauth)\n",
    "    return ((len(set(unauthLst)), len(set(withFldLst)), round(100*len(set(unauthLst))/len(set(withFldLst)))))    \n",
    "\n",
    "def unauthorisedAgents(records, fieldtags, autrefSubfield):\n",
    "    #returns the list of records having at least a field with tag in fieldtags \n",
    "    #    that do not have the subfield autrefSubfield\n",
    "    unauthLst=[]\n",
    "    withFldLst=[]\n",
    "    for fld in fieldtags:\n",
    "        withFld=selectAssigned(records,fld)\n",
    "        unauth=list(set(withFld).difference(set(selectAssigned(records, fld, autrefSubfield))))\n",
    "        withFldLst.extend(withFld)\n",
    "        unauthLst.extend(unauth)\n",
    "    return list(set(unauthLst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Poster med uautoriserte agenter:')\n",
    "print('Prøysen:', unauthorisedAgentsInfo(proysen, ['100','110','111','700', '710','711'], '0'))\n",
    "print('Solstad:', unauthorisedAgentsInfo(solstad, ['100','110','111','700', '710','711'], '0'))\n",
    "print('Collett:', unauthorisedAgentsInfo(collett, ['100','110','111','700', '710','711'], '0'))\n",
    "print('Hamsun:', unauthorisedAgentsInfo(hamsun, ['100','110','111','700', '710','711'], '0'))\n",
    "print('Bjørnson:', unauthorisedAgentsInfo(bjornson, ['100','110','111','700', '710','711'], '0'))\n",
    "#print('Undset:', unauthorisedAgentsInfo(undset, ['100','110','700', '710'], '0'))\n",
    "print('Norske bøker:', unauthorisedAgentsInfo(norskeboker, ['100','110','111','700', '710','711'], '0'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Exporting and reading all authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on Export function in API (https://authority.bibsys.no/authority/)\n",
    "#Uses a downloaded export\n",
    "expPath='C:/Users/oddruno/OneDrive - Nasjonalbiblioteket/Dokumenter/Verksregister/SHARE-VDE/Authorities export/20240202/'\n",
    "fstr='2024-01-31-010002_'\n",
    "def toStr3(num):\n",
    "    if num<10:\n",
    "        res='00'+str(num)\n",
    "    else:\n",
    "        if num<100:\n",
    "            res='0'+ str(num)\n",
    "        else:\n",
    "            res=str(num)\n",
    "    return res\n",
    "\n",
    "def toStr2(num):\n",
    "    if num<10:\n",
    "        res='0'+str(num)\n",
    "    else:\n",
    "        if num<100:\n",
    "            res=str(num)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read and parse all authorities\n",
    "authorities=[]\n",
    "#The range end point must be 1 + the number of files. \n",
    "for i in range(0,243):\n",
    "    flind=toStr3(i)\n",
    "    recs=pymarc.parse_xml_to_array(expPath + fstr + flind + '.xml')\n",
    "    authorities.extend(recs)\n",
    "\n",
    "#Extract the set which have gender info\n",
    "withGender=selectAssigned(authorities, '375')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kat1=select(authorities, '901', ['kat1']) #300740\n",
    "kat2=select(authorities, '901', ['kat2']) #1727664 \n",
    "kat3=select(authorities, '901', ['kat3']) #178327 \n",
    "print(len(kat1), len(kat2), len(kat3), 'sum:', len(kat1)+len(kat2)+len(kat3), len(authorities)) #authorities=2208423\n",
    "sumkat=len(kat1)+len(kat2)+len(kat3)\n",
    "sumtot=len(authorities)\n",
    "k1perc=round(len(kat1)*100/sumtot, 0)\n",
    "k2perc=round(len(kat2)*100/sumtot, 0)\n",
    "k3perc=round(len(kat3)*100/sumtot, 0)\n",
    "print('prosentfordeling:', k1perc, k2perc, k3perc, 'totalt:', k1perc+k2perc+k3perc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons=selectAssigned(authorities, '100')\n",
    "personsWG=selectAssigned(persons, '375')\n",
    "print(\"% of persons with gender:\", round(len(personsWG)*100/len(persons),0), '%')\n",
    "print('% of authorities which are persons:', round(len(persons)*100/len(authorities), 0), '%')\n",
    "len(persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crosscheck\n",
    "print('Number of person authorities with gender info: ', len(personsWG))\n",
    "personsWG == withGender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kat1Persons=selectAssigned(kat1, '100')\n",
    "wG1=selectAssigned(kat1Persons, '375')\n",
    "print('kat1 persons:', len(kat1Persons), 'with gender:', len(wG1), round(len(wG1)*100/len(kat1Persons),1), '%') #263973\n",
    "kat1Corps=selectAssigned(kat1, '110')\n",
    "len(kat1Corps) #29638\n",
    "kat2Persons=selectAssigned(kat2, '100')\n",
    "wG2=selectAssigned(kat2Persons, '375')\n",
    "print('kat2 persons:', len(kat2Persons),'with gender:', len(wG2), round(len(wG2)*100/len(kat2Persons),1), '%') #263973\n",
    "kat3Persons=selectAssigned(kat3, '100')\n",
    "wG3=selectAssigned(kat3Persons, '375')\n",
    "print('kat3 persons:', len(kat3Persons),'with gender:', len(wG3), round(len(wG3)*100/len(kat3Persons),1), '%') #263973"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Extract gender info from authority registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversikt over kjønnsfordeling på hovedforfattere i de ulike bibliografiene\n",
    "#Først selve postene\n",
    "#OBS: Tar 4 timer, fjern Samisk neste gang\n",
    "genderDict=dict()\n",
    "for bn in bibliografiNavn.keys():\n",
    "    genderDict[bn]=authorGender(withGender, bibliografiNavn[bn])\n",
    "    #Print to file, for easier access next time around\n",
    "    writeMarcToFile(genderDict[bn][0], bn +'-f'+'.xml')  \n",
    "    writeMarcToFile(genderDict[bn][1], bn +'-m'+'.xml')\n",
    "    writeMarcToFile(genderDict[bn][2], bn +'-noGender'+'.xml')\n",
    "#Så opptelling    \n",
    "genderDictCount=dict()\n",
    "for bn in bibliografiNavn.keys():\n",
    "    genderDictCount[bn]=list (map (lambda x: len(x), genderDict[bn]))\n",
    "genderDictCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'solstad': [171, 1539, 862],\n",
    " 'prøysen': [486, 7129, 1085],\n",
    " 'hamsun': [487, 3036, 3843],\n",
    " 'collett': [612, 171, 158],\n",
    " 'bjørnson': [129, 3803, 897],\n",
    " 'norske bøker': [72, 3767, 1459],\n",
    " '1814': [213, 2147, 1090],\n",
    " 'norsk-amerikansk': [262, 1545, 4729],\n",
    " 'samisk': [9383, 13451, 4398]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ratios of lacking gender info in autreg\n",
    "for bn in bibliografiNavn.keys():\n",
    "    noGenderRatio=round(100*genderDictCount[bn][2]/sum(genderDictCount[bn]), 0)\n",
    "    print(bn, 'no gender ratio:', noGenderRatio, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 For records where no gender info was found, check against female and male first names extracted from SSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sjekk postene der kjønn på forfatter ikke ble funnet, opp mot SSBs lister over jente- og guttenavn.\n",
    "#Les inn jentenavn og guttenavn extrahert fra SSB. \n",
    "j_ssb=readlines('Jentenavn2013-2023.txt')\n",
    "g_ssb=readlines('Guttenavn2013-2023.txt')\n",
    "\n",
    "genderDictExt=dict()\n",
    "\n",
    "for bn in bibliografiNavn.keys():\n",
    "    undecided=genderDict[bn][2]   #3rd element in result from authorGender\n",
    "    genderDictExt[bn]=authorGender2(undecided, j_ssb, g_ssb)\n",
    "\n",
    "#Så opptelling    \n",
    "genderDictExtCount=dict()\n",
    "for bn in bibliografiNavn.keys():\n",
    "    genderDictExtCount[bn]=list (map (lambda x: len(x), genderDictExt[bn]))\n",
    "genderDictExtCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Merge the results from 1 and 2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MERGE the lists as  THE  BELOW --- .extend changes genderDict!!  #############\n",
    "##genderDictMerged=dict()\n",
    "##for bn in bibliografiNavn.keys():\n",
    "##    genderDictMerged[bn]=[genderDict[bn][0].extend(genderDictExt[bn][0]), \n",
    "##                         genderDict[bn][1].extend(genderDictExt[bn][1]),\n",
    "##                         genderDictExt[bn][2]]\n",
    "\n",
    "#Merge counts directly   \n",
    "genderDictMergedCount=dict()\n",
    "for bn in bibliografiNavn.keys():\n",
    "    genderDictMergedCount[bn]=[genderDictCount[bn][0]+genderDictExtCount[bn][0],\n",
    "                              genderDictCount[bn][1]+genderDictExtCount[bn][1],\n",
    "                              genderDictExtCount[bn][2]]\n",
    "genderDictMergedCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ratios for lacking gender info, after checking against SSB data\n",
    "for bn in bibliografiNavn.keys():\n",
    "    noGenderRatioFinal=round(100*genderDictMergedCount[bn][2]/sum(genderDictMergedCount[bn]), 0)\n",
    "    print(bn, 'no gender ratio (final):', noGenderRatioFinal, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if correct. sum(genderDictMergedCount[bn]) should equal sum(genderDictCount[bn]))\n",
    "for bn in bibliografiNavn.keys():\n",
    "    if sum(genderDictMergedCount[bn]) == sum(genderDictCount[bn]):\n",
    "        print(bn, 'OK')\n",
    "    else:\n",
    "        print(bn, 'feil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=list(genderDictCount.values())\n",
    "df_gender=pd.DataFrame(list(genderDictMergedCount.values()), index=bibliografiNavn.keys(), \n",
    "                     columns=['female', 'male', 'no gender info'])\n",
    "df_gender_full=pd.DataFrame(columns=bibliografiNavn.keys(), \n",
    "                     index=['female', 'male', 'no gender info/no autid', 'no main author'])\n",
    "for bn in bibliografiNavn.keys():\n",
    "    df_gender_full[bn][:-1]=df_gender.T[bn]\n",
    "    df_gender_full[bn][-1]=len(bibliografiNavn[bn])-df_gender_full[bn][:-1].sum()\n",
    "df_gender_full\n",
    "\n",
    "df_gender_rel=pd.DataFrame(columns=bibliografiNavn.keys(), \n",
    "                     index=['female', 'male', 'no gender info/no autid', 'no main author'])\n",
    "#for bn in bibliografiNavn.keys():\n",
    "    #reldf[bn][:-1] = round((df_bibliografiskNiva[bn][:-1] / df_bibliografiskNiva[bn][-1]) * 100, 0)\n",
    "    #df_gender_rel[bn] = (df_gender_full[bn] // sum(df_gender_full[bn])) * 100\n",
    "    #df_gender_rel[bn] = (df_gender_full[bn] / df_gender_full[bn].sum()) * 100\n",
    "df_gender_full.to_csv('genderData.csv')\n",
    "df_gender_full\n",
    "\n",
    "#Plotting is made from Excel in gender.xlsx (in Notabene folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 A geographical perspective (about places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeName(placeString):\n",
    "    #Remove possible qualifiers\n",
    "    return placeString.partition('(')[0].strip()\n",
    "\n",
    "def qualifier(placeString):\n",
    "    #Remove possible qualifiers\n",
    "    tmp=placeString.partition('(')[2]\n",
    "    return tmp.partition(')')[0].strip()\n",
    "    \n",
    "noramPlDict= valueCounter(noram651,['651'], subfieldtags=['a'], countDupl=False ) #864\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB: geonameUsr is your username in Geonames (a string)\n",
    "\n",
    "def lookupPlace(plStr, featureClasses=None, fpri=None, user=geonameUsr, useQualifier=0):\n",
    "    #looks up in Geonames via geocoder the place represented by plStr (typically fetched from 651$a)\n",
    "    #if useQualifier=0, only look up placeName(plStr)\n",
    "    #if useQualifier=1, look up placeName(plStr). If not found, look up qualifier(plStr)\n",
    "    #if useQualifier=2, look up qualifier(plStr) if it exists and retruns result, else look up placeName(plStr)\n",
    "    #fpri is one featureClass to be prioritized, featureClasses defines the featureClasses to be searched amongst \n",
    "    if useQualifier ==0:\n",
    "        g=lookupPl(placeName(plStr), featureClasses=featureClasses,fpri=fpri, user=user)\n",
    "    elif useQualifier==1:\n",
    "        g=lookupPl(placeName(plStr), featureClasses=featureClasses,fpri=fpri, user=user)\n",
    "        if g.lat is None or g.lng is None:\n",
    "            if qualifier(plStr) != '':\n",
    "                g=lookupPl(qualifier(plStr), featureClasses=featureClasses,fpri=fpri, user=user)\n",
    "    elif useQualifier==2:\n",
    "        if qualifier(plStr) != '':\n",
    "            g=lookupPl(qualifier(plStr), featureClasses=featureClasses,fpri=fpri, user=user)\n",
    "            if g.lat is None or g.lng is None:\n",
    "                g=lookupPl(placeName(plStr), featureClasses=featureClasses,fpri=fpri, user=user)\n",
    "        else:\n",
    "            g=lookupPl(placeName(plStr), featureClasses=featureClasses,fpri=fpri, user=user)\n",
    "    else:\n",
    "        return None\n",
    "    return g\n",
    "\n",
    "def lookupPl(plStr, featureClasses=None, fpri=None, user=geonameUsr):\n",
    "    #looks up plStr in Geonames via geocoder\n",
    "    #tries fpri (a featureClass) first. If that fails try featureClasses\n",
    "    if fpri is not None:\n",
    "        g=geocoder.geonames(plStr, featureClass=fpri, key=user)\n",
    "        if g.lat is None or g.lng is None:\n",
    "            g=geocoder.geonames(plStr, featureClass=featureClasses, key=user)\n",
    "    else:\n",
    "        g=geocoder.geonames(plStr, featureClass=featureClasses, key=user)\n",
    "    return g   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w662=selectAssigned(noram, '662')\n",
    "w651=selectAssigned(noram, '651')\n",
    "wgeo=list(set(w651).union(set(w662)))\n",
    "print('651:', len (w651), '662:', len (w662), 'Den ene eller andre:', len(wgeo))\n",
    "set(w651) == set(wgeo)   #Alle med 662 har også 651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ratio of the documents in each bibliography that are about a geographical topic\n",
    "for k in bibliografiNavn.keys():\n",
    "    exec(bibliografiVar[k]+'651 = selectAssigned(bibliografiNavn[k], \\'651\\')')\n",
    "    exec(bibliografiVar[k]+'662 = selectAssigned(bibliografiNavn[k], \\'662\\')')\n",
    "    print(bibliografiVar[k]+'651:', round(len(eval(bibliografiVar[k]+'651'))*100/len(bibliografiNavn[k])))\n",
    "    print(bibliografiVar[k]+'662:', round(len(eval(bibliografiVar[k]+'662'))*100/len(bibliografiNavn[k])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why field 662 is not used in the geographical analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Further investigation of 662: Frequently wrong assignment of subfield codes, rendering USA as a city, for example.\n",
    "\n",
    "printFieldss(w662, ['245', '651', '662']) shows this\n",
    "Examples:\n",
    "662 :    ['d', 'USA', 'f', 'New York (staten', 'f', 'New York (byen)', '2', 'norbok']\n",
    "662 :    ['d', 'USA', 'f', 'Minnesota', '2', 'norbok']\n",
    "662 :    ['d', 'USA', 'f', 'Texas', 'f', 'Bosque County', '2', 'norbok']  ....USA as a city\n",
    "662 :    ['a', 'Kvinnherad', 'b', 'Ølve', '2', 'norbok']......................Kvinnherad as a country\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Map for geographical topics of noram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with useQualifier=2 (use qualifier first. If not found, use the placename term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1dict=dict()\n",
    "for pl in noramPlDict.keys():\n",
    "    m1dict[pl]=lookupPlace(pl,featureClasses= ['A', 'P', 'L'], fpri='A', useQualifier=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the maps\n",
    "import folium\n",
    "m1 = folium.Map(location=[62,10])    #Norway\n",
    "m1nf=[]\n",
    "for pl in noramPlDict.keys():\n",
    "    g=m1dict[pl]\n",
    "    if g.lat is None or g.lng is None:\n",
    "        m1nf.append(pl)\n",
    "    else:\n",
    "        c=noramPlDict[pl]\n",
    "        #r=500*max(1, math.sqrt(c))\n",
    "        r=500*c/2\n",
    "        folium.Circle(location=[g.lat, g.lng], radius=r, \n",
    "                      popup=pl+', '+str(c), color=\"crimson\", fill=True).add_to(m1)\n",
    "m1.save(\"noram_q2_fpri_A.html\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with useQualifier=1 (use specific name  first. If not found, use the qualifier term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1dict2=dict()\n",
    "for pl in noramPlDict.keys():\n",
    "    m1dict2[pl]=lookupPlace(pl,featureClasses= ['A', 'P', 'L'], useQualifier=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the maps, useQualifier=2\n",
    "m1 = folium.Map(location=[62,10])    #Norway\n",
    "m1nf2=[]\n",
    "for pl in noramPlDict.keys():\n",
    "    g=m1dict2[pl]\n",
    "    if g.lat is None or g.lng is None:\n",
    "        m1nf2.append(pl)\n",
    "    else:\n",
    "        c=noramPlDict[pl]\n",
    "        #r=500*max(1, math.sqrt(c))\n",
    "        r=500*c/2\n",
    "        folium.Circle(location=[g.lat, g.lng], radius=r, \n",
    "                      popup=pl+', '+str(c), color=\"crimson\", fill=True).add_to(m1)\n",
    "m1.save(\"noram_q1_fpri_None.html\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Publishing years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Norske bøker:\n",
    "publishedYears(norskeboker).plot(kind='line', figsize=(12,8))\n",
    "#1814\n",
    "publishedYears(bibl1814).plot(kind='line', figsize=(12,8))\n",
    "#norsk-amerikansk\n",
    "publishedYears(noram).plot(kind='line', figsize=(12,8))\n",
    "#hamsun\n",
    "publishedYears(hamsun).plot(kind='line', figsize=(12,8))\n",
    "#bjornson\n",
    "publishedYears(bjornson).plot(kind='line', figsize=(12,8))\n",
    "#collett\n",
    "publishedYears(collett).plot(kind='line', figsize=(12,8))\n",
    "#samisk\n",
    "publishedYears(samisk).plot(kind='line', figsize=(12,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfYear=publishedYears(bibl1814)\n",
    "figYear=dfYear.plot(kind='line', figsize=(12,8), legend=False, \n",
    "                title='Publishing year distribution in the 1814 bibliography').get_figure()\n",
    "figYear.savefig('publyear1814.pdf')\n",
    "figYear.savefig('publyear1814.jpeg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Genre and form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 Genres in the 1814 bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ordsky for sjangre 1814\n",
    "\n",
    "valueCounter(bibl1814, ['655'], subfieldtags=('a', '2') )\n",
    "medSjanger1814=filterRecords(bibl1814, '(norvok)|(bib1814)', ['655'], ['2'])\n",
    "tempDict1814=valueCounter(medSjanger1814, ['655'], subfieldtags=('a', '2'))\n",
    "#Se bare på norvok- og bib1814-sjangerord, og fjern $norvok og $bib1814'\n",
    "norvokSjangre1814=dict()\n",
    "for k in tempDict1814.keys():\n",
    "    if '$norvok' in k:\n",
    "        nk=k[:-7]   #strip $norvok\n",
    "        #some terms may have norvok in some records, bib1814 in others\n",
    "        if nk in norvokSjangre1814.keys():\n",
    "            norvokSjangre1814[nk]+=tempDict1814[k]\n",
    "        else:\n",
    "            norvokSjangre1814[nk]=tempDict1814[k]\n",
    "    if '$bib1814' in k:\n",
    "        nk=k[:-8]   #strip $norvok\n",
    "        if nk in norvokSjangre1814.keys():\n",
    "            norvokSjangre1814[nk]+=tempDict1814[k]\n",
    "        else:\n",
    "            norvokSjangre1814[nk]=tempDict1814[k]\n",
    "\n",
    "wc=WordCloud(background_color='white', width=800, height=400, mode='RGB').generate_from_frequencies(norvokSjangre1814)\n",
    "wc.to_file('wc-bibl1814.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2  Genres in Norske bøker 1539-1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ordsky for sjangre norske bøker\n",
    "\n",
    "#valueCounter(norskeboker, ['655'], subfieldtags=('a', '2') )\n",
    "medSjangerNB=filterRecords(norskeboker, 'norvok', ['655'], ['2'])\n",
    "tempDictNB=valueCounter(medSjangerNB, ['655'], subfieldtags=('a', '2'))\n",
    "#Se bare på norvok-sjangerord, og fjern $norvok. (Egen kildekode for Norske bøker er aldri opprettet)\n",
    "norvokSjangreNB=dict()\n",
    "for k in tempDictNB.keys():\n",
    "    if '$norvok' in k:\n",
    "        norvokSjangreNB[k[:-7]]=tempDictNB[k]\n",
    "\n",
    "wc=WordCloud(background_color='white', width=800, height=400, mode='RGB').generate_from_frequencies(norvokSjangreNB)\n",
    "wc.to_file('wc-norskeboker.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Of and about authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solstad\n",
    "omSolstad=filterRecords(solstad, 'Solstad, Dag.{0,2}$', ['600'], ['a', 't'])\n",
    "omSolstadsVerker=filterRecords(solstad, 'Solstad, Dag \\w+', ['600'], ['a', 't'])\n",
    "avSolstad=filterRecords(solstad, 'Solstad, Dag', ['100'])\n",
    "#fieldValues(solstad, ['008'], slice=(34,35))\n",
    "bio_s=filterRecordsByControlField(solstad, 'b|d', '008', (34,35))\n",
    "print('Solstad: Av:\\t',len(avSolstad), '\\tOm:', len(omSolstad), '\\tBiografikode:', \n",
    "      len(bio_s),'\\tOm verker:',len(omSolstadsVerker),  '\\tTotalt:', len(solstad))\n",
    "\n",
    "#Prøysen\n",
    "omProysen=filterRecords(proysen, 'Prøysen, Alf.{0,2}$', ['600'], ['a', 't'])\n",
    "omProysensVerker=filterRecords(proysen, 'Prøysen, Alf \\w+', ['600'], ['a', 't'])\n",
    "avProysen=filterRecords(proysen, 'Prøysen, Alf', ['100'])\n",
    "\n",
    "#Special handling of Prøysen, since his contributions in terms of song lyrics are not reflected in the 100 field\n",
    "#   but in the 700 field, with role = aut or lyr\n",
    "tmp=filterRecords(proysen, 'Prøysen, Alf', ['700'])\n",
    "contribProysen=filterRecords(tmp, '(lyr)|(aut)', ['700'], subfieldtags=['4'])\n",
    "avProysen= list(set(avProysen).union(set(contribProysen)))\n",
    "\n",
    "bio_p=filterRecordsByControlField(proysen, 'b|d', '008', (34,35))\n",
    "print('Prøysen: Av:\\t',len(avProysen), '\\tOm:', len(omProysen), '\\tBiografikode:', \n",
    "      len(bio_p),'\\tOm verker:',len(omProysensVerker),  '\\tTotalt:', len(proysen))\n",
    "\n",
    "#Collett\n",
    "omCollett=filterRecords(collett, 'Collett, Camilla.{0,2}$', ['600'] , ['a', 't'])\n",
    "omCollettsVerker=filterRecords(collett, 'Collett, Camilla \\w+', ['600'] , ['a', 't'])\n",
    "avCollett=filterRecords(collett, 'Collett, Camilla', ['100'])\n",
    "bio_c=filterRecordsByControlField(collett, 'b|d', '008', (34,35))\n",
    "print('Collett: Av:\\t',len(avCollett), '\\tOm:', len(omCollett), '\\tBiografikode:', \n",
    "      len(bio_c),'\\tOm verker:',len(omCollettsVerker),  '\\tTotalt:', len(collett))\n",
    "\n",
    "#Hamsun\n",
    "omHamsun=filterRecords(hamsun, 'Hamsun, Knut.{0,2}$', ['600'], ['a', 't'])\n",
    "omHamsunsVerker=filterRecords(hamsun, 'Hamsun, Knut \\w+', ['600'], ['a', 't'])\n",
    "avHamsun=filterRecords(hamsun, 'Hamsun, Knut', ['100'])\n",
    "bio_h=filterRecordsByControlField(hamsun, 'b|d', '008', (34,35))\n",
    "print('Hamsun: Av:\\t',len(avHamsun), '\\tOm:', len(omHamsun), '\\tBiografikode:', \n",
    "      len(bio_h),'\\tOm verker:',len(omHamsunsVerker),  '\\tTotalt:', len(hamsun))\n",
    "\n",
    "#Bjørnson\n",
    "omBjornson=filterRecords(bjornson, 'Bjørnson, Bjørnstjerne.{0,2}$', ['600'], ['a', 't'] )\n",
    "omBjornsonsVerker=filterRecords(bjornson, 'Bjørnson, Bjørnstjerne \\w+', ['600'], ['a', 't'] )\n",
    "avBjornson=filterRecords(bjornson, 'Bjørnson, Bjørnstjerne', ['100'])\n",
    "bio_b=filterRecordsByControlField(bjornson, 'b|d', '008', (34,35))\n",
    "print('Bjørnson: Av:\\t',len(avBjornson), '\\tOm:', len(omBjornson), '\\tBiografikode:', \n",
    "      len(bio_b),'\\tOm verker:',len(omBjornsonsVerker),  '\\tTotalt:', len(bjornson))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorDict={'Bjornson': bjornson,'Collett':collett, 'Hamsun':hamsun, 'Proysen': proysen ,'Solstad':solstad }\n",
    "sums=[]\n",
    "df_av_om=pd.DataFrame(columns=authorDict.keys(), index=['Av', 'Om', 'Om verk'])\n",
    "df_av_om_rel=pd.DataFrame(columns=authorDict.keys(), index=['Av', 'Om', 'Om verk'])\n",
    "for au in authorDict.keys():\n",
    "    sums.append(len(eval('av'+au))+len(eval('om'+au))+ len(eval('om'+au+'sVerker')))\n",
    "    df_av_om[au][0]=len(eval('av'+au))\n",
    "    df_av_om_rel[au][0]=round(df_av_om[au][0]/len(authorDict[au])*100)\n",
    "    df_av_om[au][1]=len(eval('om'+au))\n",
    "    df_av_om_rel[au][1]=round(df_av_om[au][1]/len(authorDict[au])*100)\n",
    "    df_av_om[au][2]=len(eval('om'+au+'sVerker'))\n",
    "    df_av_om_rel[au][2]=round(df_av_om[au][2]/len(authorDict[au])*100)\n",
    "sums\n",
    "df_av_om_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avomfig=df_av_om_rel.T.plot(kind='bar', figsize=(12,8), fontsize= 'xx-large',stacked=True, legend={'reverse'}).get_figure()\n",
    "plt.legend(bbox_to_anchor=(1,1), fontsize='large')\n",
    "#fig1.savefig('mtypeabs.pdf', bbox_inches='tight')\n",
    "avomfig.savefig('av-om-verk.jpeg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  About language distribution, translations, etc in the documents (not included in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversettelser\n",
    "#Kriterier for å bli definert som oversettelse (tilstrekkelig hver for seg): \n",
    "#1.Det finnes et 246-felt med $i som inneholder \"originaltittel\"\n",
    "#2.Det finnes et 041-felt med $h\n",
    "#3.Det finnes et 765-felt (original language entry)\n",
    "\n",
    "def translations(records):\n",
    "    #returns the records in records that appear to be translations\n",
    "    return list(set(filterRecords(records, '(O|originaltit)', ['246'], ['i'])).union\n",
    "                (set(selectAssigned(records, '041', ['h'])), set(selectAssigned(records,'765'))))\n",
    "\n",
    "oversettelserAndel=[]\n",
    "for bn in bibliografiNavn.keys():\n",
    "    b=bibliografiNavn[bn]\n",
    "    overs=translations(b)\n",
    "    oversettelserAndel.append((bn, len(overs), len(b), round((len(overs)*100)/len(b))))\n",
    "\n",
    "for tp in oversettelserAndel:\n",
    "    print (tp[0]+':','\\t', 'Antall oversettelser:', tp[1], '\\t', \n",
    "           'Antall poster i bibl:', tp[2],'\\t', 'Andel oversettelser', tp[3], '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Språk i dokumentene i bibliografiene\n",
    "languageDicts=[valueCounter(solstad, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(proysen, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(hamsun, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(collett, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(bjornson, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(norskeboker, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(bibl1814, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(noram, ['008', '041'], ['a'], slice=(35,38)),\n",
    "    valueCounter(samisk, ['008', '041'], ['a'], slice=(35,38))]\n",
    "\n",
    "reducedLanguageDicts=list(map(lambda x: reduceNumDict(x,50), languageDicts))\n",
    "\n",
    "languageTable=mergeDicts(languageDicts)\n",
    "reducedLanguageTable= mergeDicts(reducedLanguageDicts)\n",
    "\n",
    "\n",
    "#showMarcRecord(solstad[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeDicts(reducedLanguageDicts)\n",
    "heading=''\n",
    "for bnavn in list(bibliografiNavn):\n",
    "    heading=heading+ '\\t'+ bnavn[0:4]\n",
    "print(heading)\n",
    "for key in list(reducedLanguageTable):\n",
    "    row= key+':'\n",
    "    for n in reducedLanguageTable[key]:\n",
    "        row=row + '\\t' + str(n)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langFrame=pd.DataFrame(list(reducedLanguageTable.values()), columns=bibliografiNavn.keys(), index=reducedLanguageTable.keys())\n",
    "type(languageDicts[0])\n",
    "#red1=reduceNumDict(languageDicts[0], 20)\n",
    "#langFrame.T.plot(kind='bar', figsize=(12,8), stacked=True)\n",
    "langFrame_rel=pd.DataFrame(columns=bibliografiNavn.keys(), \n",
    "                     index=reducedLanguageTable.keys())\n",
    "for bn in bibliografiNavn.keys():\n",
    "    langFrame_rel[bn] = round((langFrame[bn] / langFrame[bn].sum()) * 100, 0)\n",
    "langFig=langFrame_rel.T.plot(kind='bar', figsize=(12,8), fontsize= 'xx-large',stacked=True, legend={'reverse'}).get_figure()\n",
    "plt.legend(bbox_to_anchor=(1,1), fontsize='large')\n",
    "#fig1.savefig('mtypeabs.pdf', bbox_inches='tight')\n",
    "langFig.savefig('langs2.jpeg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langSamisk=langFrame_rel['samisk'].plot(kind='pie', figsize=(20,20),fontsize='xx-large', legend=False).get_figure()\n",
    "#langSamisk.legend(bbox_to_anchor=(1,1), fontsize='xx-large')\n",
    "langSamisk.savefig('langSamisk.jpeg')\n",
    "#langFrame['samisk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langFrame['samisk'].plot.pie().get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overs_h=selectAssigned(norskeboker, '041', ['h'])\n",
    "len(overs_h)\n",
    "#showMarcRecord(overs_h[2])\n",
    "overs_h[2].get_fields()[4].tag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
